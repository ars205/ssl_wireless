KUL-NLOS-URA-LAB:
"realMax": 0.071470265481,
"imagMax": 0.071470265481,
"absMax": 0.0804657278138,
- For training: Complete set of transformations, as is (without flipping of local views). 
- For better transferability in spot estimation, use wd from 0.1 to 0.001. But works worse in regression.
- The other case is when wd from 0.04 to 0.8. But works worse in regression.

KUL-LOS-ULA-LAB:
"realMax": 1.96484375,
"imagMax": 1.9453125,
"absMax": 2.11407,
- For training: Complete set of transformations, without Normalization of two global views.


KUL-LOS-DIS-LAB:
"realMax": 2.015625,
"imagMax": 2.1484375,
"absMax": 2.26824000,
- For training: Complete set of transformations, without Normalization of two global views.

conf_train.json when KUL datasets:
{   
    "project_path": ".",
    "experiment_name": "KUL_LOS_ULA_NoNorm_GViews_aug_R9k_B512_E100",
    "saved_dataset_path": "/content/drive/My Drive/1. TUWien/1. A RESEARCH/8. EXPERIMENTS/KU_Luwen_Dataset",
    "dataset_to_download": "ULA_lab_LoS",
    "sub_dataset_to_use": "kuluwen_ULA_lab_LoS_CSI_9k.npy",
    "sub_loc_dataset_to_use": "kuluwen_ULA_lab_LoS_LOC_9k.npy",
    "realMax": 1.96484375,
    "imagMax": 1.9453125,
    "absMax": 2.11407,
    "Ns":8,
    "model_name": "wit",
    "global_projector_out": 25000,
    "local_projector_out": 1024,
    "momentum_encoder": 0.996,
    "k_nns": 3,
    "warmup_temp": 0.04,
    "target_temp": 0.04,
    "warmup_temp_epochs":0,
    "batch_size_per_gpu": 512,
    "epochs":100,
    "freeze_layer_num_epochs":1,
    "learning_rate":0.00015,
    "warmup_epochs_learning_rate": 10,
    "minimum_lr": 1e-06,
    "optimizer": "adamw"
}


conf_train.json when S datasets:
{   
    "project_path": ".",
    "experiment_name": "S_NoNorm_GViews_aug_R12k_B256_E100",
    "saved_dataset_path": "/content/drive/My Drive/1. TUWien/1. A RESEARCH/8. EXPERIMENTS/18_DeepRT/Datasets",
    "dataset_to_download": "S-200",
    "sub_dataset_to_use": "S_200_CSI_12k.npy",
    "sub_loc_dataset_to_use": "S_200_LOC_12k.npy",
    "realMax": 0.00019218,
    "imagMax": 0.00020084,
    "absMax": 0.00020361,
    "Ns":8,
    "model_name": "wit",
    "global_projector_out": 25000,
    "local_projector_out": 1024,
    "momentum_encoder": 0.996,
    "k_nns": 3,
    "warmup_temp": 0.04,
    "target_temp": 0.04,
    "warmup_temp_epochs":0,
    "batch_size_per_gpu": 256,
    "epochs":100,
    "freeze_layer_num_epochs":1,
    "learning_rate":0.00015,
    "warmup_epochs_learning_rate": 10,
    "minimum_lr": 1e-06,
    "optimizer": "adamw"
}

S-200:
"realMax": 0.00019218,
"imagMax": 0.00020084,
"absMax": 0.00020361,
- For training: Complete set of transformations, with (MUST) do Normalization of two global views.

HB-200:
"realMax": 0.000265611,
"imagMax": 0.000265611,
"absMax": 0.000272077,
- For training: Complete set of transformations, with (MUST) do Normalization of two global views.
